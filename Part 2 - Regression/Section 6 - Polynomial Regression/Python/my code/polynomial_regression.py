# -*- coding: utf-8 -*-
"""Copy of polynomial_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ipurOiGGeI3yNM1uTZeBZnKtg8cchvZF

# Polynomial Regression

## Importing the libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

"""## Importing the dataset"""

dataset = pd.read_csv('Position_Salaries.csv')
X = dataset.iloc[:, 1:-1].values
y = dataset.iloc[:, -1].values

#print('X: \n', X)
#print('y: \n', y)
#print('nulls num: \n', dataset.isnull().sum())

"""## Training the Linear Regression model on the whole dataset"""

from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(X,y)

"""## Training the Polynomial Regression model on the whole dataset"""

# the polynomial regression is not a linear model
# because it can learn non linear correlations (like x1^2 ... X1^n)
# see in lack of multicollinarity (Assumptions of linear regression)
# but we call it polynomial linear regression because there is linear
# combination of squared powered features (x1 (equals x1^0), x1^2 ... x1^n)
# diffrently from linear and mulitple regression (they have diffrent features
# x1...xn)
# the features in polynomial regression is the same feature but squared and powerd

from sklearn.preprocessing import PolynomialFeatures
# degree is the parameter for n (like x1^n), in 3 or 4 degree we get better results
pr = PolynomialFeatures(degree=4)

# turn matrix of *single* featurs into this new matrix of features
# composed x1 as first feature, x1^2 second and so on
X_poly = pr.fit_transform(X) # we turn the column Level to new matrix of features
# print(X_poly)

# since now we have new matrix of features we can tun it into linear regression again
lr2 = LinearRegression()
lr2.fit(X_poly,y)

"""## Visualising the Linear Regression results"""

plt.scatter(X,y, color='red')
plt.plot(X,lr.predict(X), color='blue')
plt.title('Truth or Bluff (Linear Regression)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()

"""## Visualising the Polynomial Regression results"""

plt.scatter(X,y, color='red')
plt.plot(X,lr2.predict(pr.fit_transform(X)), color='blue')
plt.title('Truth or Bluff (Polynomial Regression)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()

"""## Visualising the Polynomial Regression results (for higher resolution and smoother curve)"""

#instead level integers 0-10 increase density of these points
# from 1, 1.1 ... 9.9, 10
X_grid = np.arange(min(X), max(X), 0.1)
X_grid = X_grid.reshape((len(X_grid), 1))
plt.scatter(X, y, color = 'red')
plt.plot(X_grid, lr2.predict(pr.fit_transform(X_grid)), color = 'blue')
plt.title('Truth or Bluff (Polynomial Regression)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()

"""## Predicting a new result with Linear Regression"""

# the predicted result is 6.5 as seen in lessons
lr.predict([[6.5]])
# the result is way over from that salary that should have
# for person in level 6.5 (between 6 and 7)

"""## Predicting a new result with Polynomial Regression"""

lr2.predict(pr.fit_transform([[6.5]])) # new matrix of features as mentioned
# the result is close to real salary this person on his level earned

"""### coefficent and intercept of to linear regressions

"""

print("lr coef: ",lr.coef_)
print("lr intercept: ",lr.intercept_)

print("lr2 coef: ",lr2.coef_)
print("lr2 intercept: ",lr2.intercept_)