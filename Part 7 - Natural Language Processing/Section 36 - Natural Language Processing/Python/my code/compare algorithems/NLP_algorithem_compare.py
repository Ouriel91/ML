# -*- coding: utf-8 -*-
"""Copy of natural_language_processing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1APdfvrMggEOlBtsIyrE0IK-7m6icbe7w

# Natural Language Processing

## Importing the libraries
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

"""## Importing the dataset"""

dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\t', quoting = 3)

"""## Cleaning the texts"""

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

corpus = []

for i in range(0, len(dataset)):
  clean_review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])
  clean_review = clean_review.lower()
  clean_review = clean_review.split()
  ps = PorterStemmer()
  all_stopwords = stopwords.words('english')
  all_stopwords.remove('not')
  clean_review = [ps.stem(word) for word in clean_review if not word in set(all_stopwords)]
  clean_review = ' '.join(clean_review)
  corpus.append(clean_review)

#arr = np.array(corpus)
#newarr = arr.reshape(len(corpus), 1)
#print(newarr)

"""## Creating the Bag of Words model"""

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features=1500)
X = cv.fit_transform(corpus).toarray()
y = dataset.iloc[:, -1].values

#print(len(X[0]))

"""## Splitting the dataset into the Training set and Test set"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

"""## Training the models on the Training set"""

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

models = [LogisticRegression(random_state = 0),
          KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2),
          SVC(kernel = 'linear', random_state = 0),
          SVC(kernel = 'rbf', random_state = 0),
          GaussianNB(),
          DecisionTreeClassifier(criterion='entropy', random_state=0),
          RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
          ]
fits = []
for i in range (0, len(models)):
  fits.append(models[i].fit(X_train, y_train))

#print(fits)

"""## Predicting the Test set results"""

predictions = []
for i in range (0, len(fits)):
  predictions.append(fits[i].predict(X_test))
#print(predictions)

"""## Making the Confusion Matrix



"""

models_names=['Logistic Regression',
              'KNeighborsClassifier',
              'Support Vector Machine',
              'Kernel SVM',
              'Naive Bayes',
              'DecisionTree',
              'RandomForest']

for i in range(0, len(predictions)):
  print("{}:".format(models_names[i]))
  print('confusion matrix:\n',confusion_matrix(y_test, predictions[i]))
  print('\n')

"""## Making table of models scores"""

from tabulate import tabulate

table_data = []
"""
data = [["Mavs", 99,1,1],
        ["Suns", 91],
        ["Spurs", 94],
        ["Nets", 88]
"""
data = []
for i in range(0, len(predictions)):
  content_data = [
      models_names[i],
      "{:.2f}".format(accuracy_score(y_test, predictions[i])*100),
      "{:.2f}".format(precision_score(y_test, predictions[i])*100),
      "{:.2f}".format(recall_score(y_test, predictions[i])*100),
      "{:.2f}".format(f1_score(y_test, predictions[i])*100)
      ]
  data.append(content_data)
col_names = ['Accuracy', 'Precision', 'Recall', 'F1']
print(tabulate(data, headers=col_names))